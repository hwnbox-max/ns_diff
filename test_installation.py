"""
Installation Test Script
éªŒè¯æ‰€æœ‰ä¾èµ–å’Œæ¨¡å—æ˜¯å¦æ­£ç¡®å®‰è£…
"""

import sys
import importlib
from typing import List, Tuple


def check_import(module_name: str, package_name: str = None) -> Tuple[bool, str]:
    """
    æ£€æŸ¥æ¨¡å—æ˜¯å¦å¯ä»¥å¯¼å…¥

    Args:
        module_name: è¦å¯¼å…¥çš„æ¨¡å—å
        package_name: æ˜¾ç¤ºçš„åŒ…å (å¦‚æœä¸æ¨¡å—åä¸åŒ)
    Returns:
        (success, message): æˆåŠŸæ ‡å¿—å’Œæ¶ˆæ¯
    """
    try:
        importlib.import_module(module_name)
        version = ""
        if hasattr(importlib.import_module(module_name), '__version__'):
            version = f" v{importlib.import_module(module_name).__version__}"
        return True, f"âœ“ {package_name or module_name}{version}"
    except ImportError as e:
        return False, f"âœ— {package_name or module_name}: {str(e)}"


def test_basic_imports():
    """æµ‹è¯•åŸºç¡€ä¾èµ–"""
    print("\n" + "=" * 60)
    print("TESTING BASIC DEPENDENCIES")
    print("=" * 60)

    packages = [
        ('torch', 'PyTorch'),
        ('torchvision', 'torchvision'),
        ('numpy', 'NumPy'),
        ('scipy', 'SciPy'),
        ('sklearn', 'scikit-learn'),
        ('PIL', 'Pillow'),
        ('matplotlib', 'Matplotlib'),
        ('pandas', 'Pandas'),
        ('tqdm', 'tqdm'),
    ]

    results = []
    for module, name in packages:
        success, msg = check_import(module, name)
        results.append(success)
        print(msg)

    return all(results)


def test_diffusion_imports():
    """æµ‹è¯•æ‰©æ•£æ¨¡å‹ç›¸å…³ä¾èµ–"""
    print("\n" + "=" * 60)
    print("TESTING DIFFUSION MODEL DEPENDENCIES")
    print("=" * 60)

    packages = [
        ('diffusers', 'Diffusers'),
        ('transformers', 'Transformers'),
        ('accelerate', 'Accelerate'),
    ]

    results = []
    for module, name in packages:
        success, msg = check_import(module, name)
        results.append(success)
        print(msg)

    return all(results)


def test_project_imports():
    """æµ‹è¯•é¡¹ç›®æ¨¡å—"""
    print("\n" + "=" * 60)
    print("TESTING PROJECT MODULES")
    print("=" * 60)

    modules = [
        'models.ns_diff',
        'models.baselines',
        'data.datasets',
        'evaluation.metrics',
        'evaluation.visualization',
    ]

    results = []
    for module in modules:
        try:
            importlib.import_module(module)
            print(f"âœ“ {module}")
            results.append(True)
        except ImportError as e:
            print(f"âœ— {module}: {str(e)}")
            results.append(False)

    return all(results)


def test_model_instantiation():
    """æµ‹è¯•æ¨¡å‹å®ä¾‹åŒ–"""
    print("\n" + "=" * 60)
    print("TESTING MODEL INSTANTIATION")
    print("=" * 60)

    try:
        import torch
        from models.ns_diff_error import NSDiff
        from models.baselines import build_model

        # æµ‹è¯•NS-Diff
        print("Creating NS-Diff model...")
        model = NSDiff(
            num_concepts=8,
            num_classes=2,
            latent_dim=512
        )
        num_params = sum(p.numel() for p in model.parameters())
        print(f"âœ“ NS-Diff created successfully ({num_params / 1e6:.2f}M parameters)")

        # æµ‹è¯•å‰å‘ä¼ æ’­
        print("Testing forward pass...")
        x = torch.randn(2, 3, 256, 256)
        outputs = model(x)
        print(f"âœ“ Forward pass successful")
        print(f"  - Predictions shape: {outputs['predictions'].shape}")
        print(f"  - Concepts shape: {outputs['concepts'].shape}")

        # æµ‹è¯•åäº‹å®ç”Ÿæˆ
        print("Testing counterfactual generation...")
        x_cf, info = model.generate_counterfactual(x[:1], target_concept_idx=0, target_value=1.0)
        print(f"âœ“ Counterfactual generation successful")
        print(f"  - Generated image shape: {x_cf.shape}")

        # æµ‹è¯•åŸºçº¿æ¨¡å‹
        print("\nTesting baseline models...")
        baselines = ['resnet50', 'standard_cbm', 'posthoc_cbm', 'disdiff_fnnc']
        for baseline in baselines:
            try:
                model = build_model(baseline, num_concepts=8, num_classes=2)
                print(f"âœ“ {baseline} created successfully")
            except Exception as e:
                print(f"âœ— {baseline}: {str(e)}")
                return False

        return True

    except Exception as e:
        print(f"âœ— Model instantiation failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½ (éœ€è¦å®é™…æ•°æ®æ–‡ä»¶)"""
    print("\n" + "=" * 60)
    print("TESTING DATA LOADING")
    print("=" * 60)

    try:
        from data.datasets import Shapes3DDataset, CelebAHQDataset
        import torch
        import numpy as np

        # åˆ›å»ºè™šæ‹Ÿæ•°æ®æµ‹è¯•
        print("Testing dataset classes...")

        # æ³¨æ„: è¿™é‡Œåªæµ‹è¯•ç±»å®šä¹‰,ä¸æµ‹è¯•å®é™…æ•°æ®åŠ è½½
        print("âœ“ Dataset classes imported successfully")
        print("  Note: Actual data loading requires dataset files")

        return True

    except Exception as e:
        print(f"âœ— Data loading test failed: {str(e)}")
        return False


def test_metrics():
    """æµ‹è¯•è¯„ä¼°æŒ‡æ ‡"""
    print("\n" + "=" * 60)
    print("TESTING EVALUATION METRICS")
    print("=" * 60)

    try:
        from evaluation.metrics import compute_mig, compute_mutual_information
        import numpy as np

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        print("Testing MIG computation...")
        n_samples = 1000
        n_concepts = 3
        n_factors = 3

        # æ¨¡æ‹Ÿå®Œç¾è§£è€¦
        factors = np.random.randint(0, 10, size=(n_samples, n_factors))
        concepts = np.zeros((n_samples, n_concepts))
        for i in range(n_concepts):
            concepts[:, i] = factors[:, i] / 10.0

        mig = compute_mig(concepts, factors)
        print(f"âœ“ MIG computation successful: {mig:.4f}")

        if mig > 0.5:
            print("  MIG value looks reasonable for disentangled data")
        else:
            print("  Warning: MIG value lower than expected")

        return True

    except Exception as e:
        print(f"âœ— Metrics test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def test_visualization():
    """æµ‹è¯•å¯è§†åŒ–"""
    print("\n" + "=" * 60)
    print("TESTING VISUALIZATION")
    print("=" * 60)

    try:
        from evaluation.visualization import plot_performance_comparison
        import matplotlib
        matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯

        print("Testing performance comparison plot...")
        results = {
            'Model A': {'accuracy': 90.0, 'mig': 0.5, 'isr': 80.0},
            'Model B': {'accuracy': 88.0, 'mig': 0.6, 'isr': 75.0}
        }

        plot_performance_comparison(results, save_path='/tmp/test_plot.png')
        print("âœ“ Visualization test successful")

        return True

    except Exception as e:
        print(f"âœ— Visualization test failed: {str(e)}")
        return False


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 80)
    print("NS-DIFF INSTALLATION TEST SUITE")
    print("=" * 80)

    tests = [
        ("Basic Dependencies", test_basic_imports),
        ("Diffusion Dependencies", test_diffusion_imports),
        ("Project Modules", test_project_imports),
        ("Model Instantiation", test_model_instantiation),
        ("Data Loading", test_data_loading),
        ("Evaluation Metrics", test_metrics),
        ("Visualization", test_visualization),
    ]

    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"\nâœ— {test_name} crashed: {str(e)}")
            results[test_name] = False

    # æ€»ç»“
    print("\n" + "=" * 80)
    print("TEST SUMMARY")
    print("=" * 80)

    for test_name, success in results.items():
        status = "âœ“ PASSED" if success else "âœ— FAILED"
        print(f"{test_name:.<40}{status}")

    total_passed = sum(results.values())
    total_tests = len(results)

    print("\n" + "=" * 80)
    print(f"OVERALL: {total_passed}/{total_tests} tests passed")

    if total_passed == total_tests:
        print("ğŸ‰ All tests passed! Installation is complete.")
        print("\nYou can now:")
        print("  1. Run training: python train.py --help")
        print("  2. Run experiments: python experiments/run_all_experiments.py")
    else:
        print("âš ï¸  Some tests failed. Please check the errors above.")
        print("\nCommon fixes:")
        print("  1. Install missing packages: pip install -r requirements.txt")
        print("  2. Check PYTHONPATH includes project root")
        print("  3. Ensure CUDA is properly installed (if using GPU)")

    print("=" * 80 + "\n")

    return total_passed == total_tests


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)